# Turbo-Tune ğŸš€

A high-performance MoE (Mixture of Experts) finetuning framework optimized for ultimate training efficiency. This project implements cutting-edge techniques to maximize training speed and reduce memory usage for large language models.

## ğŸ¯ Key Features

- **Ultra-fast MoE training** with optimized scatter operations
- **Multiple precision formats** (FP8, BF16) for memory efficiency
- **Advanced distributed training** with DeepSpeed ZeRO and FSDP
- **Kernel optimizations** with Liger Kernel and Flash Attention
- **Comprehensive efficiency monitoring** with real-time metrics

## ğŸ› ï¸ Efficiency Technologies

### 1. **Fast MoE Implementation**
- **ScatterMoE**: Custom implementation of scatter operations for MoE blocks
- **Parallel Expert Computation**: Optimized parallel matrix multiplications across experts
- **Load Balancing**: Auxiliary loss for router load balancing
- **Memory-Efficient Routing**: Sorted expert indices and padded block operations

### 2. **Precision Optimization**
- **FP8 Training**: 8-bit floating point precision using PyTorch AO (Accelerated Optimization)
- **BF16 Mixed Precision**: Brain Float 16 for numerical stability
- **Selective FP8**: Applied to first/last linear layers and router gates only
- **Flash Attention 2**: Optimized attention implementation for packing and FP8

### 3. **Distributed Training**
- **DeepSpeed ZeRO**: Stage 1, 2, and 3 optimizations
  - ZeRO-1: Optimizer state partitioning
  - ZeRO-2: Gradient partitioning
  - ZeRO-3: Parameter partitioning with prefetching
- **FSDP (Fully Sharded Data Parallel)**: 
  - Backward prefetching
  - Forward prefetching
  - Activation checkpointing
- **Multi-node Support**: Distributed training across multiple nodes

### 4. **Kernel Optimizations**
- **Liger Kernel**: High-performance CUDA kernels for transformer operations
- **Torch Compile**: Just-in-time compilation for model optimization
- **Custom SiLU Operations**: Optimized SwiGLU activation functions
- **Triton Kernels**: GPU-optimized scatter operations

### 5. **Memory Management**
- **Gradient Checkpointing**: Memory-efficient gradient computation
- **Packing**: Sequence packing for efficient token processing
- **Completion-only Loss**: Loss computation only on completion tokens
- **Dynamic Memory Tracking**: Real-time memory usage monitoring

### 6. **Training Optimizations**
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning
- **Gradient Accumulation**: Large effective batch sizes with small micro-batches
- **Cosine Learning Rate Scheduling**: Optimal learning rate decay
- **Warmup Strategy**: Gradual learning rate warmup

### 7. **Efficiency Monitoring**
- **Real-time Metrics**: Step time, memory usage, tokens per second
- **MLflow Integration**: Comprehensive experiment tracking
- **Token Accuracy Tracking**: Per-step token prediction accuracy
- **Memory Profiling**: Peak allocated and reserved memory tracking

## ğŸ“ Project Structure

```
turbo-tune/
â”œâ”€â”€ qwen3_sft.py              # Main training script
â”œâ”€â”€ qwen3_sft.sh              # Single-node training script
â”œâ”€â”€ qwen3_sft_multinode.sh    # Multi-node training script
â”œâ”€â”€ callback.py               # Efficiency monitoring callback
â”œâ”€â”€ config/                   # Training configurations
â”‚   â”œâ”€â”€ fsdp.json            # FSDP configuration
â”‚   â”œâ”€â”€ zero1.json           # DeepSpeed ZeRO-1 config
â”‚   â”œâ”€â”€ zero2.json           # DeepSpeed ZeRO-2 config
â”‚   â””â”€â”€ zero3.json           # DeepSpeed ZeRO-3 config
â””â”€â”€ monkey_patch/            # Optimized implementations
    â”œâ”€â”€ fast_moe.py          # Fast MoE block implementation
    â”œâ”€â”€ fp8.py               # FP8 training patches
    â”œâ”€â”€ sft_trainer.py       # Optimized SFT trainer
    â””â”€â”€ scattermoe/          # Scatter operations
        â”œâ”€â”€ ops.py           # Core scatter operations
        â”œâ”€â”€ kernels.py       # CUDA kernels
        â””â”€â”€ compilable_ops.py # Compilable operations
```

## ğŸš€ Quick Start

### Single Node Training
```bash
# Set your paths
export MODEL_PATH="path/to/your/model"
export DATA_PATH="path/to/your/dataset"
export CKPT_PREFIX="path/to/checkpoints"
export DS_CONFIG_PATH="config/zero3.json"

# Run training with all optimizations
bash qwen3_sft.sh
```

### Multi-Node Training
```bash
# Set your paths and network configuration
export MODEL_PATH="path/to/your/model"
export DATA_PATH="path/to/your/dataset"
export CKPT_PREFIX="path/to/checkpoints"
export DS_CONFIG_PATH="config/zero3.json"
export MASTER_ADDR="your.master.ip"
export MASTER_PORT="29500"

# Run distributed training
bash qwen3_sft_multinode.sh
```

## âš™ï¸ Configuration Options

### Training Parameters
- `--use_fast_moe_block`: Enable optimized MoE implementation
- `--use_fp8`: Enable FP8 training
- `--use_liger_kernel`: Enable Liger Kernel optimizations
- `--torch_compile`: Enable PyTorch compilation
- `--use_packing`: Enable sequence packing
- `--use_lora`: Enable LoRA fine-tuning

### Distributed Training
- `--deepspeed_config`: DeepSpeed configuration file
- `--fsdp`: FSDP configuration (e.g., "full_shard auto_wrap")
- `--global_batch_size`: Total batch size across all devices
- `--micro_batch_size`: Batch size per device

### Efficiency Monitoring
- `--use_liger_efficiency_callback`: Enable efficiency metrics logging
- `--logging_steps`: Logging frequency (set to 1 for detailed monitoring)

## ğŸ“Š Performance Metrics

The framework tracks comprehensive efficiency metrics:
- **Step Time**: Time per training step
- **Tokens per Second**: Training throughput
- **Memory Usage**: Peak allocated and reserved memory
- **Token Accuracy**: Per-step prediction accuracy
- **Time to Completion**: Estimated training completion time

## ğŸ”§ Dependencies

Install all required dependencies:

```bash
pip install -r requirements.txt
```

For GPU support, ensure you have CUDA installed and use the appropriate PyTorch version for your CUDA version.

## ğŸ“ˆ Performance Benefits

- **2-3x faster training** with optimized MoE operations
- **50-70% memory reduction** with FP8 and distributed training
- **Improved throughput** with kernel optimizations
- **Better convergence** with advanced scheduling and loss functions